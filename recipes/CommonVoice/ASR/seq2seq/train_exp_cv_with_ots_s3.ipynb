{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1240c4b-2e92-4cfb-8a15-8a5a441b7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d88779-5ff3-4e53-90f6-75cb579ce7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.10.1+cu102\n",
      "torchaudio version: 0.10.1+cu102\n"
     ]
    }
   ],
   "source": [
    "# check the torch and torchaudio version\n",
    "print('torch version: {}'.format(torch.__version__))\n",
    "print('torchaudio version: {}'.format(torchaudio.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad44785-cc53-4a7d-b5c9-98c2bcf97b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the bucket name\n",
    "bucket_name = 'zge-exp'\n",
    "\n",
    "# specify the S3 file path to the file to be downloaded\n",
    "file_path_s3 = 'data/cv-corpus-8.0-2022-01-19/fr/clips/common_voice_fr_27042964.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d0b5ac-4d42-4afb-be5d-b81ea218a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup s3 client\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91980b9d-ecc4-459e-b0dc-60bc09ba3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio file from S3 (without downloading)\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_path_s3)\n",
    "waveform, sample_rate = torchaudio.load(response['Body'], format='mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b537cf45-142a-46aa-8b17-b09cbf8a106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5378e-05,\n",
      "         -3.3975e-06, -1.8001e-05]])\n"
     ]
    }
   ],
   "source": [
    "print(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167d5b17-5e02-4b65-9942-96ae6f74b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir: /root/speechbrain/recipes/CommonVoice\n"
     ]
    }
   ],
   "source": [
    "# setup work dir\n",
    "dir_sb = '/root/speechbrain'\n",
    "dir_work = os.path.join(dir_sb, 'recipes/CommonVoice')\n",
    "os.chdir(dir_work)\n",
    "print('current dir: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963075a9-4b90-4aa4-bde6-26ce63adb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "import torchaudio\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from speechbrain.tokenizers.SentencePiece import SentencePiece\n",
    "from speechbrain.utils.data_utils import undo_padding\n",
    "from speechbrain.utils.distributed import run_on_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add9607e-c867-4cd6-9f28-101bad5b1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Define training procedure\n",
    "class ASR(sb.core.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "\n",
    "        # Forward pass\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        feats = self.modules.normalize(feats, wav_lens)\n",
    "\n",
    "        ## Add augmentation if specified\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.hparams, \"augmentation\"):\n",
    "                feats = self.hparams.augmentation(feats)\n",
    "\n",
    "        x = self.modules.enc(feats.detach())\n",
    "        e_in = self.modules.emb(tokens_bos)  # y_in bos + tokens\n",
    "        h, _ = self.modules.dec(e_in, x, wav_lens)\n",
    "        # Output layer for seq2seq log-probabilities\n",
    "        logits = self.modules.seq_lin(h)\n",
    "        p_seq = self.hparams.log_softmax(logits)\n",
    "\n",
    "        # Compute outputs\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            current_epoch = self.hparams.epoch_counter.current\n",
    "            if current_epoch <= self.hparams.number_of_ctc_epochs:\n",
    "                # Output layer for ctc log-probabilities\n",
    "                logits = self.modules.ctc_lin(x)\n",
    "                p_ctc = self.hparams.log_softmax(logits)\n",
    "                return p_ctc, p_seq, wav_lens\n",
    "            else:\n",
    "                return p_seq, wav_lens\n",
    "        else:\n",
    "            p_tokens, scores = self.hparams.beam_searcher(x, wav_lens)\n",
    "            return p_seq, wav_lens, p_tokens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss (CTC+NLL) given predictions and targets.\"\"\"\n",
    "\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if current_epoch <= self.hparams.number_of_ctc_epochs:\n",
    "                p_ctc, p_seq, wav_lens = predictions\n",
    "            else:\n",
    "                p_seq, wav_lens = predictions\n",
    "        else:\n",
    "            p_seq, wav_lens, predicted_tokens = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        loss_seq = self.hparams.seq_cost(\n",
    "            p_seq, tokens_eos, length=tokens_eos_lens\n",
    "        )\n",
    "\n",
    "        # Add ctc loss if necessary\n",
    "        if (\n",
    "            stage == sb.Stage.TRAIN\n",
    "            and current_epoch <= self.hparams.number_of_ctc_epochs\n",
    "        ):\n",
    "            loss_ctc = self.hparams.ctc_cost(\n",
    "                p_ctc, tokens, wav_lens, tokens_lens\n",
    "            )\n",
    "            loss = self.hparams.ctc_weight * loss_ctc\n",
    "            loss += (1 - self.hparams.ctc_weight) * loss_seq\n",
    "        else:\n",
    "            loss = loss_seq\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Decode token terms to words\n",
    "            predicted_words = self.tokenizer(\n",
    "                predicted_tokens, task=\"decode_from_list\"\n",
    "            )\n",
    "\n",
    "            # Convert indices to words\n",
    "            target_words = undo_padding(tokens, tokens_lens)\n",
    "            target_words = self.tokenizer(target_words, task=\"decode_from_list\")\n",
    "\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Train the parameters given a single batch in input\"\"\"\n",
    "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
    "        loss.backward()\n",
    "        if self.check_gradients(loss):\n",
    "            self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss.detach()\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        predictions = self.compute_forward(batch, stage=stage)\n",
    "        with torch.no_grad():\n",
    "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.cer_metric = self.hparams.cer_computer()\n",
    "            self.wer_metric = self.hparams.error_rate_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
    "        # Compute/store important stats\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
    "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"loss\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"WER\": stage_stats[\"WER\"]}, min_keys=[\"WER\"],\n",
    "            )\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stage_stats,\n",
    "            )\n",
    "            with open(self.hparams.wer_file, \"w\") as w:\n",
    "                self.wer_metric.write_stats(w)\n",
    "\n",
    "\n",
    "# Define custom data procedure\n",
    "def dataio_prepare(hparams, tokenizer):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "\n",
    "    # 1. Define datasets\n",
    "    data_folder = hparams[\"data_folder\"]\n",
    "\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"train_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        # we sort training data to speed up training and get better results.\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\",\n",
    "            key_max_value={\"duration\": hparams[\"avoid_if_longer_than\"]},\n",
    "        )\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        train_data = train_data.filtered_sorted(\n",
    "            sort_key=\"duration\",\n",
    "            reverse=True,\n",
    "            key_max_value={\"duration\": hparams[\"avoid_if_longer_than\"]},\n",
    "        )\n",
    "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
    "        hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    # We also sort the validation data so it is faster to validate\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"test_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "\n",
    "    # We also sort the validation data so it is faster to validate\n",
    "    test_data = test_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "    datasets = [train_data, valid_data, test_data]\n",
    "\n",
    "    # 2. Define audio pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        sig, info = sb.dataio.dataio.read_audio(wav)\n",
    "        if info[\"num_channels\"] > 1:\n",
    "            sig = torch.mean(sig, dim=1)\n",
    "        resampled = torchaudio.transforms.Resample(\n",
    "            info[\"sample_rate\"], hparams[\"sample_rate\"],\n",
    "        )(sig)\n",
    "        return resampled\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "    # 3. Define text pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"wrd\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
    "    )\n",
    "    def text_pipeline(wrd):\n",
    "        tokens_list = tokenizer.sp.encode_as_ids(wrd)\n",
    "        #tokens_list = tokenizer.encode_as_ids(wrd)\n",
    "        yield tokens_list\n",
    "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
    "        yield tokens_bos\n",
    "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
    "        yield tokens_eos\n",
    "        tokens = torch.LongTensor(tokens_list)\n",
    "        yield tokens\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
    "\n",
    "    # 4. Set output:\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets, [\"id\", \"sig\", \"tokens_bos\", \"tokens_eos\", \"tokens\"],\n",
    "    )\n",
    "    return train_data, valid_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "761c2612-7db4-47ac-8243-98b75334be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments: ['ASR/seq2seq/hparams/train_fr_exp_cv_with_ots.yml', '--batch_size=2']\n"
     ]
    }
   ],
   "source": [
    "argvs = ['ASR/seq2seq/hparams/train_fr_exp_cv_with_ots.yml', '--batch_size=2']\n",
    "print('arguments: {}'.format(argvs))\n",
    "hparams_file, run_opts, overrides = sb.parse_arguments(argvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1170ed-eda8-4c8f-81ef-fb108f90a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f0c2ad-f965-448b-8bc1-0878aa9df248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/cv_with_ots_fr_seq2seq_ctc_attention_now2v_nosmooth\n"
     ]
    }
   ],
   "source": [
    "# If distributed_launch=True then\n",
    "# create ddp_group with the right communication protocol\n",
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "# Dataset preparation (parsing CommonVoice)\n",
    "# from common_voice_prepare_fr import prepare_common_voice  # noqa\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides=overrides,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d15e80-34fa-44a0-992a-69375b57e9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.tokenizers.SentencePiece - Tokenizer is already trained.\n",
      "speechbrain.tokenizers.SentencePiece - ==== Loading Tokenizer ===\n",
      "speechbrain.tokenizers.SentencePiece - Tokenizer path: results/cv_with_ots_fr_seq2seq_ctc_attention_now2v_nosmooth/save/500_unigram.model\n",
      "speechbrain.tokenizers.SentencePiece - Tokenizer vocab_size: 500\n",
      "speechbrain.tokenizers.SentencePiece - Tokenizer type: unigram\n"
     ]
    }
   ],
   "source": [
    "# Defining tokenizer and loading it\n",
    "tokenizer = SentencePiece(\n",
    "    model_dir=hparams[\"save_folder\"],\n",
    "    vocab_size=hparams[\"output_neurons\"],\n",
    "    annotation_train=hparams[\"train_csv\"],\n",
    "    annotation_read=\"wrd\",\n",
    "    model_type=hparams[\"token_type\"],\n",
    "    character_coverage=hparams[\"character_coverage\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9917ce18-12a1-4421-b786-9af85ad8aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets objects as well as tokenization and encoding :-D\n",
    "train_data, valid_data, test_data = dataio_prepare(hparams, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e81d29b-cb37-425b-bf9d-16ba1eda61a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0192e-05, -6.7353e-06, -4.1723e-07,  2.3842e-06,  1.6093e-06,\n",
       "         1.3709e-06,  2.2650e-06,  3.5763e-06,  2.6226e-06, -2.0266e-06,\n",
       "        -3.9935e-06,  3.5763e-07,  2.9206e-06,  1.7881e-07, -2.3842e-06,\n",
       "        -4.4703e-06, -7.2718e-06, -8.2254e-06, -9.1195e-06, -1.0669e-05,\n",
       "        -7.9274e-06, -4.1127e-06, -5.6028e-06, -6.0797e-06, -2.9802e-07,\n",
       "         4.3511e-06,  3.8743e-06,  3.2187e-06,  5.3644e-06,  9.4175e-06,\n",
       "         1.1623e-05,  6.1989e-06, -6.7353e-06, -1.7941e-05, -2.0146e-05,\n",
       "        -1.6868e-05, -1.2994e-05, -6.3181e-06,  2.5034e-06,  5.7220e-06,\n",
       "         6.5565e-06,  1.2040e-05,  1.3173e-05,  5.1856e-06,  1.1921e-06,\n",
       "         1.4901e-06, -2.5630e-06, -2.3842e-07,  1.2875e-05,  1.8179e-05,\n",
       "         1.4067e-05,  1.1921e-05,  5.0068e-06, -4.8876e-06,  1.1921e-07,\n",
       "         1.2517e-05,  1.1504e-05,  2.7418e-06, -5.9605e-07,  8.3447e-07,\n",
       "         3.5763e-06,  2.9802e-06, -4.5896e-06, -1.5318e-05, -3.1710e-05,\n",
       "        -5.8770e-05, -8.3447e-05, -9.2924e-05, -9.2387e-05, -8.4460e-05,\n",
       "        -6.2168e-05, -3.1054e-05, -8.2850e-06, -8.9407e-07, -5.9605e-07,\n",
       "        -2.9802e-07, -4.8876e-06, -1.2875e-05, -6.4373e-06,  1.7345e-05,\n",
       "         3.9577e-05,  5.6505e-05,  7.5221e-05,  8.2493e-05,  6.5506e-05,\n",
       "         3.6895e-05,  1.2338e-05, -5.3644e-06, -1.1206e-05, -5.9605e-07,\n",
       "         1.4961e-05,  1.9073e-05,  1.2577e-05,  2.8014e-06, -1.6987e-05,\n",
       "        -5.1260e-05, -8.4221e-05, -1.0252e-04, -1.0461e-04, -8.5652e-05])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = sb.dataio.dataio.read_audio(train_data.data[train_data.data_ids[3]]['wav'])\n",
    "sig[8000:8100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96828387-2635-4c6c-a1af-cb8e567620d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 bucket path: s3://zge-exp/data/cv-corpus-8.0-2022-01-19/fr/clips/common_voice_fr_27042964.mp3\n"
     ]
    }
   ],
   "source": [
    "file_path_bucket = 's3://{}'.format(os.path.join(bucket_name, file_path_s3))\n",
    "print('s3 bucket path: {}'.format(file_path_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5bd427e-2482-4bb0-8f3e-27086f2f944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket name: zge-exp\n",
      "S3 path: data/cv-corpus-8.0-2022-01-19/fr/clips/common_voice_fr_27042964.mp3\n",
      "ext: mp3\n"
     ]
    }
   ],
   "source": [
    "waveforms_obj = file_path_bucket\n",
    "s3_prefix = 's3://'\n",
    "len_s3_prefix = len(s3_prefix)\n",
    "bucket_name = waveforms_obj[len(s3_prefix):].split(os.sep)[0]\n",
    "len_bucket_name = len(bucket_name)\n",
    "s3_path = waveforms_obj[len_s3_prefix+len_bucket_name+len(os.sep):]\n",
    "ext = os.path.splitext(s3_path)[1]\n",
    "print('bucket name: {}'.format(bucket_name))\n",
    "print('S3 path: {}'.format(s3_path))\n",
    "print('ext: {}'.format(ext[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce2ed16d-123e-4794-8bbb-ac5cc280bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio shape: torch.Size([1, 193536])\n",
      "sample rate: 32000\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=s3_path)\n",
    "audio, sr = torchaudio.load(response['Body'], format=ext[1:])\n",
    "print('audio shape: {}'.format(audio.shape))\n",
    "print('sample rate: {}'.format(sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ba428b-24d8-4aee-8957-2341dfd6c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.8247e-05,  8.0228e-05,  4.6909e-05, -6.5565e-07,  1.7703e-05,\n",
      "         3.2663e-05, -1.1206e-05, -1.6510e-05,  1.8716e-05,  1.0967e-05,\n",
      "        -5.9605e-07,  1.0848e-05, -1.3113e-06, -1.3590e-05,  1.6689e-06,\n",
      "         1.6093e-05,  1.7524e-05,  1.1206e-05,  2.6286e-05,  6.2048e-05,\n",
      "         4.1842e-05, -1.1742e-05, -7.1526e-07,  1.7703e-05, -1.1206e-05,\n",
      "        -1.0014e-05,  2.0862e-06, -3.1650e-05, -4.5896e-05, -2.7776e-05,\n",
      "        -2.3901e-05, -1.3411e-05, -1.2517e-06, -6.0201e-06, -5.0664e-06,\n",
      "        -1.7583e-05, -2.7895e-05,  2.1994e-05,  6.6876e-05,  4.8637e-05,\n",
      "         2.9445e-05,  2.1458e-05,  1.7464e-05,  4.0650e-05,  2.9683e-05,\n",
      "        -1.6451e-05, -1.1563e-05, -2.0742e-05, -7.2062e-05, -2.7359e-05,\n",
      "         4.7803e-05, -5.3644e-07, -4.1544e-05,  1.8477e-05,  4.4286e-05,\n",
      "         2.2531e-05,  2.2888e-05, -1.0729e-06, -3.9518e-05, -3.7968e-05,\n",
      "        -4.6134e-05, -6.4611e-05, -2.5570e-05,  1.8060e-05, -8.5235e-06,\n",
      "        -5.4538e-05, -7.4446e-05, -7.7188e-05, -4.0591e-05,  1.7822e-05,\n",
      "         2.0266e-05, -9.9540e-06,  4.2319e-06,  1.9848e-05, -7.3910e-06,\n",
      "        -1.4544e-05,  1.2517e-06, -2.0206e-05, -4.6313e-05, -3.0160e-05,\n",
      "        -4.1723e-06,  6.7949e-06,  3.7551e-06, -1.8120e-05, -3.8743e-05,\n",
      "        -2.9325e-05,  1.2219e-05,  5.8770e-05,  6.4433e-05,  1.9789e-05,\n",
      "        -2.1219e-05, -1.2398e-05,  2.3425e-05,  2.4855e-05, -1.6034e-05,\n",
      "        -3.7074e-05, -2.0146e-05, -1.2279e-05, -2.4259e-05, -3.1114e-05])\n"
     ]
    }
   ],
   "source": [
    "sig = audio.transpose(0, 1).squeeze(1)\n",
    "print(sig[8000:8100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b667de6-1eb1-47d9-997c-63f0ef1851c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: {'sample_rate': 32000, 'num_channels': 1}\n",
      "tensor([ 6.8247e-05,  8.0228e-05,  4.6909e-05, -6.5565e-07,  1.7703e-05,\n",
      "         3.2663e-05, -1.1206e-05, -1.6510e-05,  1.8716e-05,  1.0967e-05,\n",
      "        -5.9605e-07,  1.0848e-05, -1.3113e-06, -1.3590e-05,  1.6689e-06,\n",
      "         1.6093e-05,  1.7524e-05,  1.1206e-05,  2.6286e-05,  6.2048e-05,\n",
      "         4.1842e-05, -1.1742e-05, -7.1526e-07,  1.7703e-05, -1.1206e-05,\n",
      "        -1.0014e-05,  2.0862e-06, -3.1650e-05, -4.5896e-05, -2.7776e-05,\n",
      "        -2.3901e-05, -1.3411e-05, -1.2517e-06, -6.0201e-06, -5.0664e-06,\n",
      "        -1.7583e-05, -2.7895e-05,  2.1994e-05,  6.6876e-05,  4.8637e-05,\n",
      "         2.9445e-05,  2.1458e-05,  1.7464e-05,  4.0650e-05,  2.9683e-05,\n",
      "        -1.6451e-05, -1.1563e-05, -2.0742e-05, -7.2062e-05, -2.7359e-05,\n",
      "         4.7803e-05, -5.3644e-07, -4.1544e-05,  1.8477e-05,  4.4286e-05,\n",
      "         2.2531e-05,  2.2888e-05, -1.0729e-06, -3.9518e-05, -3.7968e-05,\n",
      "        -4.6134e-05, -6.4611e-05, -2.5570e-05,  1.8060e-05, -8.5235e-06,\n",
      "        -5.4538e-05, -7.4446e-05, -7.7188e-05, -4.0591e-05,  1.7822e-05,\n",
      "         2.0266e-05, -9.9540e-06,  4.2319e-06,  1.9848e-05, -7.3910e-06,\n",
      "        -1.4544e-05,  1.2517e-06, -2.0206e-05, -4.6313e-05, -3.0160e-05,\n",
      "        -4.1723e-06,  6.7949e-06,  3.7551e-06, -1.8120e-05, -3.8743e-05,\n",
      "        -2.9325e-05,  1.2219e-05,  5.8770e-05,  6.4433e-05,  1.9789e-05,\n",
      "        -2.1219e-05, -1.2398e-05,  2.3425e-05,  2.4855e-05, -1.6034e-05,\n",
      "        -3.7074e-05, -2.0146e-05, -1.2279e-05, -2.4259e-05, -3.1114e-05])\n"
     ]
    }
   ],
   "source": [
    "sig, info = sb.dataio.dataio.read_audio(file_path_bucket, return_info=True)\n",
    "print('info: {}'.format(info))\n",
    "print(sig[8000:8100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d20495-9bca-4e17-83b0-15dde84e0d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../templates/speech_recognition/filelists/cv_with_ots/old/test_tiny.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = hparams[\"test_csv\"]\n",
    "data_folder = hparams['data_folder']\n",
    "replacements={\"data_root\": data_folder}\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c395bd4-d814-49f2-af2d-1f2bcd8814c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "csvfile = open(csv_path, newline='')\n",
    "result = {}\n",
    "reader = csv.DictReader(csvfile, skipinitialspace=True)\n",
    "variable_finder = re.compile(r\"\\$([\\w.]+)\")\n",
    "for row in reader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f1f8201-f4de-4946-b294-77c4a8a8e1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'common_voice_fr_19611490',\n",
       " 'duration': '4.56',\n",
       " 'wav': '$data_root/CommonVoice/cv-corpus-8.0-2022-01-19/fr/clips/common_voice_fr_19611490.mp3',\n",
       " 'spk_id': '2faccf61dc761c9405b237051807fe80e8c8d0507da7998b11327da03217e892d3a9b130d9ffca8216a0a8a8944a95aaa6d7e0978bcfcf0413c4dbbfccdcd4e5',\n",
       " 'wrd': 'LES DEUX FRÈRES ÉTABLISSENT LEUR RÉSIDENCE À OHLAU'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecd30677-a6e7-4f82-ad6d-162c594f6271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../templates/speech_recognition/filelists/cv_with_ots/old/test_tiny.csv'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41545d84-d173-46bd-a2c4-65b9a070e531",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smart_open'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a7572a13b070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smart_open'"
     ]
    }
   ],
   "source": [
    "from smart_open import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bf6f5e5-dfa9-4d07-8ac2-c82666400c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<csv.DictReader at 0x7f15f2e9c160>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611522af-3dac-43d7-9e90-48491617f666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
